# TrustyAI Guardrailing and Evaluation Demos
ğŸ‘‹ Welcome! This repo contains demos showcasing TrustyAI's guardrailing and model evaluation features within Red Hat Openshift AI.


## ğŸ‘©â€ğŸ”¬ Evaluation Demos ğŸ“Š
* [Evaluation Quickstart](./eval-quickstart-demo/README.md): This demo will quickly get you started running an evaluation against a deployed model. 


## ğŸ’‚ğŸ¿â€â™€ï¸ Guardrail Demos ğŸ›¡ï¸
* [Guardrails Quickstart](./guardrails-quickstart-demo/README.md): This demo will quickly get you started with three detectors, for detecting hate speech, gibberish, and jailbreaking respectively.  
* [Custom Detectors](./custom-detectors/README.md): This demo shows off how to create custom detectors via Python, and provides an example of LLM self-reflection guardrailing.
* [Lemonade Stand](./lemonade-stand-demo/README.md): A demo showing manual configuration of guardrails, as shown in the [Guardrails for AI models video on the RH YouTube channel](https://www.youtube.com/watch?v=Owr2ToxRbho)
* [NeMo-Guardrails Quickstart](./nemo-guardrails-quickstart/README.md): This demo will quickly get you started
using NVIDIA's NeMo-Guardrails project to perform guardrailing on OpenShift AI.
* [Language Detector Demo](./guardrails-language-detector-demo/README.md): Attacking LLMs using non-English prompts is a common approach. This demo showing how to use a language classification model to guardrail against these kinds of attacks. 

# Troubleshooting
If you run into issues, see the [troubleshooting guide](docs/troubleshooting.md) for common issues and their solutions.
